{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive HMT Training: Understanding BPTT and Multi-Stage Learning\n",
    "\n",
    "**Paper**: *Hierarchical Memory Transformer for Efficient Long Context Language Processing*  \n",
    "**arXiv**: 2405.06067v3 [cs.CL] 6 Feb 2025\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Learning Objectives\n",
    "\n",
    "This notebook provides an **interactive deep dive** into training the Hierarchical Memory Transformer. You will understand:\n",
    "\n",
    "1. **BPTT (Backpropagation Through Time)**: How gradients flow through time-unrolled segments\n",
    "2. **Multi-Stage Training** (Appendix F): Why HMT trains in two stages and their trade-offs\n",
    "3. **Gradient Stability** (Appendix J): Why HMT is more stable than RMT\n",
    "4. **Memory Hierarchy**: How sensory, short-term, and long-term memory interact\n",
    "5. **SOTA PyTorch Practices**: Modern training loops, checkpointing, and monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## 📖 Key Paper Sections\n",
    "\n",
    "- **Section 3**: HMT Architecture\n",
    "- **Algorithm 1**: HMT Training Procedure  \n",
    "- **Appendix D**: Training Details and Hyperparameters (Table 7)\n",
    "- **Appendix F**: Multi-stage Training Methodology\n",
    "- **Appendix J**: Gradient Stability Analysis\n",
    "\n",
    "---\n",
    "\n",
    "**⚠️ Note**: This notebook is designed for **step-by-step execution**. Read the explanations in each section before running the code to maximize learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Introduction & Setup\n",
    "\n",
    "## 1.1 Understanding the HMT Architecture\n",
    "\n",
    "HMT wraps any decoder-only transformer (GPT-2, LLaMA, etc.) with a **three-level memory hierarchy**:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    HMT Architecture                          │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                              │\n",
    "│  Input Sequence (length >> L)                               │\n",
    "│         ↓                                                    │\n",
    "│  ┌──────────────────┐                                       │\n",
    "│  │ Segment into L   │ ← Short-term Memory (current segment) │\n",
    "│  │  token chunks    │                                       │\n",
    "│  └────────┬─────────┘                                       │\n",
    "│           ↓                                                  │\n",
    "│  ┌──────────────────────────────────────────┐               │\n",
    "│  │  For each segment n:                    │               │\n",
    "│  │                                          │               │\n",
    "│  │  1. Retrieve sensory memory (k tokens)  │ ← Sensory Mem │\n",
    "│  │     from segment n-1                    │   (k=32)      │\n",
    "│  │                                          │               │\n",
    "│  │  2. Encode segment → S_n                │ ← Repr Encoder│\n",
    "│  │     (RepresentationEncoder, Eq. 1)      │               │\n",
    "│  │                                          │               │\n",
    "│  │  3. Search long-term cache → P_n        │ ← Long-term   │\n",
    "│  │     (MemorySearch, Eq. 2-3)             │   Memory      │\n",
    "│  │     Cross-attention over N embeddings   │   (N=300)     │\n",
    "│  │                                          │               │\n",
    "│  │  4. Augment: [k_n || H_n || P_n]        │ ← Augmented   │\n",
    "│  │                                          │   Input       │\n",
    "│  │                                          │               │\n",
    "│  │  5. Process through backbone (GPT-2)    │ ← Backbone    │\n",
    "│  │                                          │   Model       │\n",
    "│  │                                          │               │\n",
    "│  │  6. Generate memory embedding m_n       │ ← Mem Gen     │\n",
    "│  │     (MemoryEmbeddingGenerator, Eq. 4)   │               │\n",
    "│  │                                          │               │\n",
    "│  │  7. Update long-term cache (FIFO)       │ ← Cache Update│\n",
    "│  │     M ← [M || m_n], max size N          │               │\n",
    "│  │                                          │               │\n",
    "│  │  8. Save last k tokens → sensory memory │ ← Sensory Upd │\n",
    "│  │                                          │               │\n",
    "│  └──────────────────────────────────────────┘               │\n",
    "│           ↓                                                  │\n",
    "│  Output (logits for next-token prediction)                  │\n",
    "│                                                              │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Complexity Insight (Paper Section 3)\n",
    "\n",
    "- **Standard Transformer**: O(L²) attention over full context\n",
    "- **HMT**: O(L) per segment + O(N) memory retrieval\n",
    "- **Result**: Can process contexts **much longer** than backbone's original limit\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Import Required Libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for imports\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# HMT imports\n",
    "from hmt import HMT, HMTConfig\n",
    "from hmt.trainer import HMTTrainer\n",
    "from hmt.training_config import TrainingConfig, get_opt_350m_config\n",
    "from hmt.training_utils import (\n",
    "    ExponentialDecayScheduler,\n",
    "    GradientMonitor,\n",
    "    compute_gradient_stats,\n",
    "    compute_perplexity,\n",
    "    set_seed,\n",
    "    count_parameters\n",
    ")\n",
    "from hmt.utils import get_device\n",
    "from hmt.memory import RepresentationEncoder, MemorySearch, MemoryEmbeddingGenerator\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {get_device()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 Setup Device and Seed for Reproducibility\n",
    "device = get_device()\n",
    "set_seed(42)  # Reproducibility\n",
    "\n",
    "print(f\"🖥️  Device: {device}\")\n",
    "print(f\"🌱 Random seed: 42\")\n",
    "\n",
    "if device == \"mps\":\n",
    "    print(\"   Using Apple Silicon MPS acceleration!\")\n",
    "elif device == \"cuda\":\n",
    "    print(f\"   Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"   Using CPU (training will be slower)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Load Small GPT-2 Model (Fast for Learning)\n",
    "print(\"Loading GPT-2 backbone model...\")\n",
    "backbone = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have pad token\n",
    "\n",
    "# Model stats\n",
    "param_count = sum(p.numel() for p in backbone.parameters())\n",
    "print(f\"✅ GPT-2 loaded: {param_count:,} parameters\")\n",
    "print(f\"   Hidden size: {backbone.config.hidden_size}\")\n",
    "print(f\"   Vocab size: {backbone.config.vocab_size}\")\n",
    "print(f\"   Max position embeddings: {backbone.config.n_positions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5 Create HMT Configuration\n",
    "# Paper Reference: Table 7 (Appendix D) for hyperparameters\n",
    "\n",
    "hmt_config = HMTConfig(\n",
    "    segment_length=256,           # L: Length of each segment (smaller for fast learning)\n",
    "    representation_length=128,    # j: Tokens for encoding (L/2, auto-set)\n",
    "    num_memory_embeddings=50,     # N: Long-term cache size (smaller for visualization)\n",
    "    sensory_memory_size=16,       # k: Tokens from previous segment (smaller for speed)\n",
    "    hidden_dim=768,               # Auto-detected from GPT-2\n",
    ")\n",
    "\n",
    "print(\"🔧 HMT Configuration:\")\n",
    "print(f\"   Segment Length (L): {hmt_config.segment_length}\")\n",
    "print(f\"   Representation Length (j): {hmt_config.representation_length}\")\n",
    "print(f\"   Memory Cache Size (N): {hmt_config.num_memory_embeddings}\")\n",
    "print(f\"   Sensory Memory (k): {hmt_config.sensory_memory_size}\")\n",
    "print(f\"   Hidden Dim: {hmt_config.hidden_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.6 Instantiate HMT Model\n",
    "hmt_model = HMT(backbone, hmt_config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "params = count_parameters(hmt_model)\n",
    "extra_params = params['trainable'] - param_count\n",
    "percentage = (extra_params / param_count) * 100\n",
    "\n",
    "print(\"✅ HMT Model Created!\")\n",
    "print(f\"   Backbone parameters: {param_count:,}\")\n",
    "print(f\"   HMT extra parameters: {extra_params:,} ({percentage:.2f}%)\")\n",
    "print(f\"   Total trainable: {params['trainable']:,}\")\n",
    "print(\"\\n📊 HMT Components:\")\n",
    "print(f\"   ✓ RepresentationEncoder\")\n",
    "print(f\"   ✓ MemorySearch\")\n",
    "print(f\"   ✓ MemoryEmbeddingGenerator\")\n",
    "print(f\"   ✓ Backbone (GPT-2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Understanding BPTT (Backpropagation Through Time)\n",
    "\n",
    "## 2.1 What is BPTT?\n",
    "\n",
    "**Paper Reference**: Appendix D - Training with BPTT\n",
    "\n",
    "BPTT is essential for training HMT because the model processes sequences **segment by segment**, and gradients must flow **backwards through time** across multiple segments.\n",
    "\n",
    "### Standard Transformer vs HMT\n",
    "\n",
    "```\n",
    "Standard Transformer:\n",
    "┌─────────────────────────────────────────┐\n",
    "│  Full Sequence (e.g., 2048 tokens)     │\n",
    "│         ↓                               │\n",
    "│  O(L²) Self-Attention                  │ ← Memory: O(L²)\n",
    "│         ↓                               │   Compute: O(L²)\n",
    "│  Loss & Backprop                        │\n",
    "└─────────────────────────────────────────┘\n",
    "\n",
    "HMT with BPTT:\n",
    "┌─────────────────────────────────────────┐\n",
    "│  Segment 1    Segment 2    Segment 3    │\n",
    "│  (L=256)      (L=256)      (L=256)      │\n",
    "│     ↓            ↓            ↓          │\n",
    "│  Process → M₁ Process → M₂ Process → M₃ │ ← Sequential\n",
    "│     ↓            ↓            ↓          │\n",
    "│  Loss₁       Loss₂       Loss₃          │\n",
    "│     ↑            ↑            ↑          │\n",
    "│  ────────────────────────────────────   │ ← BPTT: Gradients\n",
    "│  Backprop through K segments            │   flow backwards\n",
    "└─────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "- **K = BPTT unroll depth**: How many segments to backprop through\n",
    "- **Stage 1 (Appendix F)**: K=2 (faster, simpler)\n",
    "- **Stage 2 (Appendix F)**: K=15 (slower, learns long-range dependencies)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Visualize BPTT Computation Graph\n",
    "\n",
    "def visualize_bptt_graph(num_segments=4, unroll_depth=3):\n",
    "    \"\"\"\n",
    "    Visualize BPTT computation graph showing gradient flow.\n",
    "    \n",
    "    Paper: Appendix D - BPTT unroll depth parameter\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Draw segments\n",
    "    for i in range(num_segments):\n",
    "        x = i * 3\n",
    "        \n",
    "        # Segment box\n",
    "        color = 'lightblue' if i < unroll_depth else 'lightgray'\n",
    "        rect = plt.Rectangle((x, 2), 2, 1.5, \n",
    "                            facecolor=color, \n",
    "                            edgecolor='black', \n",
    "                            linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x+1, 2.75, f'Segment {i+1}\\n(L={hmt_config.segment_length})', \n",
    "               ha='center', va='center', fontsize=10, weight='bold')\n",
    "        \n",
    "        # Memory embedding\n",
    "        circle = plt.Circle((x+1, 1), 0.3, \n",
    "                          facecolor='orange', \n",
    "                          edgecolor='black', \n",
    "                          linewidth=2)\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(x+1, 1, f'M{i+1}', \n",
    "               ha='center', va='center', fontsize=9, weight='bold')\n",
    "        \n",
    "        # Arrow from segment to memory\n",
    "        ax.arrow(x+1, 2, 0, -0.6, \n",
    "                head_width=0.15, head_length=0.1, \n",
    "                fc='black', ec='black')\n",
    "        \n",
    "        # Forward arrow to next segment\n",
    "        if i < num_segments - 1:\n",
    "            ax.arrow(x+2.2, 2.75, 0.6, 0, \n",
    "                    head_width=0.15, head_length=0.1, \n",
    "                    fc='green', ec='green', linewidth=2)\n",
    "            ax.text(x+2.5, 3.2, 'Forward', \n",
    "                   ha='center', fontsize=8, color='green', weight='bold')\n",
    "        \n",
    "        # Backward arrow (BPTT)\n",
    "        if i > 0 and i <= unroll_depth:\n",
    "            ax.arrow(x-0.2, 2.5, -0.6, 0, \n",
    "                    head_width=0.15, head_length=0.1, \n",
    "                    fc='red', ec='red', linewidth=2, \n",
    "                    linestyle='--')\n",
    "            ax.text(x-0.5, 2.1, 'BPTT', \n",
    "                   ha='center', fontsize=8, color='red', weight='bold')\n",
    "    \n",
    "    # Legend\n",
    "    ax.text(1, 4.5, f'BPTT Unroll Depth K = {unroll_depth}', \n",
    "           fontsize=12, weight='bold', \n",
    "           bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    ax.text(num_segments*3/2, 0.2, \n",
    "           'Blue = Gradients flow through these segments\\n' + \n",
    "           'Gray = Detached (no gradient flow)', \n",
    "           ha='center', fontsize=9, \n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
    "    \n",
    "    ax.set_xlim(-1, num_segments * 3)\n",
    "    ax.set_ylim(0, 5)\n",
    "    ax.axis('off')\n",
    "    ax.set_title('BPTT Computation Graph for HMT Training', \n",
    "                fontsize=14, weight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize\n",
    "print(\"📊 BPTT Computation Graph\")\n",
    "print(\"   Green arrows: Forward pass (process segments left to right)\")\n",
    "print(\"   Red dashed arrows: Backward pass (gradients flow right to left)\")\n",
    "print(\"   Orange circles: Memory embeddings generated at each segment\\n\")\n",
    "\n",
    "visualize_bptt_graph(num_segments=5, unroll_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Compare Different BPTT Unroll Depths\n",
    "\n",
    "def compare_bptt_depths():\n",
    "    \"\"\"\n",
    "    Compare Stage 1 (K=2) vs Stage 2 (K=15) BPTT.\n",
    "    \n",
    "    Paper: Appendix F - Multi-stage training uses different unroll depths\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
    "    \n",
    "    configs = [\n",
    "        {'depth': 2, 'title': 'Stage 1: K=2 (Faster, Simpler)', 'ax': axes[0]},\n",
    "        {'depth': 15, 'title': 'Stage 2: K=15 (Slower, Long-range)', 'ax': axes[1]}\n",
    "    ]\n",
    "    \n",
    "    for cfg in configs:\n",
    "        ax = cfg['ax']\n",
    "        depth = cfg['depth']\n",
    "        \n",
    "        # Simplified visualization\n",
    "        segments = np.arange(depth)\n",
    "        gradient_strength = 1.0 / (1 + segments)  # Gradients decay over time\n",
    "        \n",
    "        bars = ax.bar(segments, gradient_strength, \n",
    "                     color=plt.cm.RdYlGn_r(gradient_strength))\n",
    "        \n",
    "        ax.set_xlabel('Segment Index (going backwards in time)', fontsize=11)\n",
    "        ax.set_ylabel('Gradient Strength', fontsize=11)\n",
    "        ax.set_title(cfg['title'], fontsize=12, weight='bold')\n",
    "        ax.set_xticks(segments)\n",
    "        ax.set_ylim(0, 1.2)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add gradient flow annotation\n",
    "        ax.annotate('Most recent\\n(strongest gradient)', \n",
    "                   xy=(0, gradient_strength[0]), \n",
    "                   xytext=(depth*0.3, 1.1),\n",
    "                   arrowprops=dict(arrowstyle='->', lw=2, color='red'),\n",
    "                   fontsize=9, ha='center')\n",
    "        \n",
    "        if depth > 5:\n",
    "            ax.annotate('Distant past\\n(weaker gradient)', \n",
    "                       xy=(depth-1, gradient_strength[-1]), \n",
    "                       xytext=(depth*0.7, 0.5),\n",
    "                       arrowprops=dict(arrowstyle='->', lw=2, color='blue'),\n",
    "                       fontsize=9, ha='center')\n",
    "    \n",
    "    plt.suptitle('BPTT Unroll Depth Comparison (Appendix F)', \n",
    "                fontsize=14, weight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n💡 Key Insights:\")\n",
    "    print(\"   • Stage 1 (K=2): Shorter gradient paths → faster training, less memory\")\n",
    "    print(\"   • Stage 2 (K=15): Longer gradient paths → learns long-range dependencies\")\n",
    "    print(\"   • Gradients naturally decay with distance (vanishing gradient problem)\")\n",
    "    print(\"   • HMT is more stable than RMT (Appendix J) due to memory architecture\")\n",
    "\n",
    "compare_bptt_depths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Multi-Stage Training Deep Dive\n",
    "\n",
    "## 3.1 Why Two Stages?\n",
    "\n",
    "**Paper Reference**: Appendix F - Multi-stage Training Methodology\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Training HMT involves learning **two complex skills simultaneously**:\n",
    "1. **Segment Encoding**: How to compress L tokens into memory embeddings\n",
    "2. **Memory Retrieval**: How to retrieve relevant memories via cross-attention\n",
    "\n",
    "Learning both at once is **difficult and slow**. Solution: **Stage the learning**!\n",
    "\n",
    "### Stage 1: Learn Segment Encoding (Lines 1-8, Appendix F)\n",
    "\n",
    "```\n",
    "Simplified Architecture (NO memory retrieval):\n",
    "┌────────────────────────────────────────────┐\n",
    "│  Input Segment (L tokens)                  │\n",
    "│       ↓                                     │\n",
    "│  [sensory_memory || segment]               │\n",
    "│       ↓                                     │\n",
    "│  Backbone Model (GPT-2)                    │\n",
    "│       ↓                                     │\n",
    "│  Generate memory embedding m_n             │\n",
    "│       ↓                                     │\n",
    "│  Store in cache M ← [M || m_n]             │\n",
    "└────────────────────────────────────────────┘\n",
    "\n",
    "Characteristics:\n",
    "  • BPTT unroll: K = 2 segments\n",
    "  • Training speed: ~1.15 s/iteration (fast!)\n",
    "  • Focus: Learn to encode segments into useful memories\n",
    "  • Steps: 200 (from paper experiments)\n",
    "```\n",
    "\n",
    "### Stage 2: Learn Memory Retrieval (Lines 9-15, Appendix F)\n",
    "\n",
    "```\n",
    "Complete Architecture (WITH memory retrieval):\n",
    "┌────────────────────────────────────────────┐\n",
    "│  Input Segment (L tokens)                  │\n",
    "│       ↓                                     │\n",
    "│  Encode → S_n (RepresentationEncoder)      │\n",
    "│       ↓                                     │\n",
    "│  Search cache → P_n (MemorySearch)         │ ← NEW!\n",
    "│       ↓                                     │\n",
    "│  [sensory || segment || retrieved_memory]  │\n",
    "│       ↓                                     │\n",
    "│  Backbone Model (GPT-2)                    │\n",
    "│       ↓                                     │\n",
    "│  Generate memory embedding m_n             │\n",
    "│       ↓                                     │\n",
    "│  Store in cache M ← [M || m_n]             │\n",
    "└────────────────────────────────────────────┘\n",
    "\n",
    "Characteristics:\n",
    "  • BPTT unroll: K = 15 segments (GPU limit)\n",
    "  • Training speed: ~3.36 s/iteration (slower)\n",
    "  • Focus: Learn cross-attention for retrieval\n",
    "  • Steps: 500 (from paper experiments)\n",
    "  • Loads checkpoint from Stage 1!\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Create Training Configuration for Multi-Stage\n",
    "\n",
    "# Paper Reference: Table 7 (Appendix D), Appendix F\n",
    "training_config = TrainingConfig(\n",
    "    backbone_model_name=\"gpt2\",\n",
    "    \n",
    "    # Multi-stage settings (Appendix F)\n",
    "    use_multi_stage=True,\n",
    "    stage1_steps=50,           # Reduced for notebook (paper: 200)\n",
    "    stage2_steps=100,          # Reduced for notebook (paper: 500)\n",
    "    stage1_bptt_segments=2,    # Paper: K=2 for Stage 1\n",
    "    stage2_bptt_segments=4,    # Reduced for notebook (paper: K=15)\n",
    "    \n",
    "    # Optimization (Table 7)\n",
    "    learning_rate=1e-5,\n",
    "    lr_decay_factor=0.9,       # 0.9 for OPT models\n",
    "    lr_decay_steps=100,\n",
    "    batch_size=1,              # Small for learning/visualization\n",
    "    gradient_clip_norm=1.0,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=5,\n",
    "    eval_steps=25,\n",
    "    save_steps=50,\n",
    "    output_dir=\"./checkpoints_interactive\",\n",
    "    \n",
    "    # Device\n",
    "    device=str(device),\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"🔧 Training Configuration Created:\")\n",
    "print(f\"\\n📊 Multi-Stage Settings:\")\n",
    "print(f\"   Stage 1: {training_config.stage1_steps} steps, K={training_config.stage1_bptt_segments}\")\n",
    "print(f\"   Stage 2: {training_config.stage2_steps} steps, K={training_config.stage2_bptt_segments}\")\n",
    "print(f\"   Total: {training_config.get_total_steps()} steps\")\n",
    "print(f\"\\n⚙️  Optimizer:\")\n",
    "print(f\"   Learning rate: {training_config.learning_rate:.0e}\")\n",
    "print(f\"   LR decay: {training_config.lr_decay_factor} every {training_config.lr_decay_steps} steps\")\n",
    "print(f\"   Batch size: {training_config.batch_size}\")\n",
    "print(f\"   Gradient clip: {training_config.gradient_clip_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Create Mock Dataset for Learning\n",
    "# (For production, use WikiText-103 or RedPajama)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MockTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Mock dataset for educational purposes.\n",
    "    For real training, use experiments/scripts/prepare_wikitext.py\n",
    "    \"\"\"\n",
    "    def __init__(self, num_samples=100, seq_length=512, vocab_size=50257):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Generate random token IDs\n",
    "        input_ids = torch.randint(0, self.vocab_size, (self.seq_length,))\n",
    "        attention_mask = torch.ones(self.seq_length, dtype=torch.long)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MockTextDataset(num_samples=100, seq_length=512)\n",
    "eval_dataset = MockTextDataset(num_samples=20, seq_length=512)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=training_config.batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, \n",
    "    batch_size=training_config.batch_size, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"📚 Mock Dataset Created:\")\n",
    "print(f\"   Train samples: {len(train_dataset)}\")\n",
    "print(f\"   Eval samples: {len(eval_dataset)}\")\n",
    "print(f\"   Sequence length: {train_dataset.seq_length}\")\n",
    "print(f\"\\n⚠️  Note: Using random data for speed. For real training, use WikiText-103.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Gradient Flow Analysis\n",
    "\n",
    "## 4.1 Why Gradient Monitoring Matters\n",
    "\n",
    "**Paper Reference**: Appendix J - Gradient Stability in HMT and RMT\n",
    "\n",
    "The paper shows that **HMT has better gradient stability than RMT** (Recurrent Memory Transformer). We'll verify this by monitoring gradients during training.\n",
    "\n",
    "### Key Problems to Detect:\n",
    "\n",
    "1. **Gradient Explosion**: Gradients become very large → training instability\n",
    "   - Threshold: grad_norm > 10.0\n",
    "   - Solution: Gradient clipping (max_norm=1.0)\n",
    "\n",
    "2. **Gradient Vanishing**: Gradients become very small → no learning\n",
    "   - Threshold: grad_norm < 1e-6  \n",
    "   - Common in deep BPTT\n",
    "\n",
    "3. **NaN/Inf Gradients**: Numerical instability\n",
    "   - Usually from division by zero or overflow\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Setup Gradient Monitor\n",
    "\n",
    "gradient_monitor = GradientMonitor(\n",
    "    window_size=20,           # Track last 20 steps for moving average\n",
    "    explosion_threshold=10.0,  # Alert if grad_norm > 10\n",
    "    vanishing_threshold=1e-6,  # Alert if grad_norm < 1e-6\n",
    ")\n",
    "\n",
    "print(\"📈 Gradient Monitor Initialized\")\n",
    "print(f\"   Window size: {gradient_monitor.window_size}\")\n",
    "print(f\"   Explosion threshold: {gradient_monitor.explosion_threshold}\")\n",
    "print(f\"   Vanishing threshold: {gradient_monitor.vanishing_threshold}\")\n",
    "print(\"\\n💡 We'll track gradient norms during training to ensure stability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Memory System Visualization\n",
    "\n",
    "## 5.1 Three-Level Memory Hierarchy\n",
    "\n",
    "Before training, let's visualize how the memory system works on a sample input.\n",
    "\n",
    "**Paper Reference**: Section 3 - HMT Architecture\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Visualize Memory Cache Evolution\n",
    "\n",
    "def visualize_memory_evolution(model, num_segments=8):\n",
    "    \"\"\"\n",
    "    Visualize how memory cache grows and FIFO behavior.\n",
    "    \n",
    "    Paper: Section 3.3 - Memory cache with max size N\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.clear_memory()\n",
    "    \n",
    "    cache_sizes = []\n",
    "    sensory_active = []\n",
    "    \n",
    "    # Process multiple segments\n",
    "    for i in range(num_segments):\n",
    "        # Create input (1 segment)\n",
    "        seq_len = hmt_config.segment_length\n",
    "        input_ids = torch.randint(0, 1000, (1, seq_len)).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, use_memory=True)\n",
    "        \n",
    "        # Track memory state\n",
    "        stats = model.get_memory_stats()\n",
    "        cache_sizes.append(stats['cache_size'])\n",
    "        sensory_active.append(1 if stats['sensory_memory_active'] else 0)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "    \n",
    "    # Cache size evolution\n",
    "    ax1 = axes[0]\n",
    "    segments = np.arange(1, num_segments + 1)\n",
    "    ax1.plot(segments, cache_sizes, marker='o', linewidth=2, markersize=8, \n",
    "            color='orange', label='Cache Size')\n",
    "    ax1.axhline(y=hmt_config.num_memory_embeddings, \n",
    "               color='red', linestyle='--', linewidth=2, \n",
    "               label=f'Max Cache Size (N={hmt_config.num_memory_embeddings})')\n",
    "    ax1.set_xlabel('Segment Number', fontsize=11)\n",
    "    ax1.set_ylabel('Cache Size (Number of Embeddings)', fontsize=11)\n",
    "    ax1.set_title('Long-Term Memory Cache Growth', fontsize=12, weight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "    ax1.set_ylim(0, hmt_config.num_memory_embeddings + 5)\n",
    "    \n",
    "    # Sensory memory activation\n",
    "    ax2 = axes[1]\n",
    "    ax2.bar(segments, sensory_active, color='lightblue', edgecolor='black', linewidth=1.5)\n",
    "    ax2.set_xlabel('Segment Number', fontsize=11)\n",
    "    ax2.set_ylabel('Sensory Memory Active', fontsize=11)\n",
    "    ax2.set_title('Sensory Memory Activation', fontsize=12, weight='bold')\n",
    "    ax2.set_ylim(0, 1.2)\n",
    "    ax2.set_yticks([0, 1])\n",
    "    ax2.set_yticklabels(['Inactive', 'Active'])\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n📊 Memory System Observations:\")\n",
    "    print(f\"   • Cache grows linearly until max size N={hmt_config.num_memory_embeddings}\")\n",
    "    print(f\"   • After reaching N, FIFO evicts oldest memories\")\n",
    "    print(f\"   • Sensory memory activates after first segment\")\n",
    "    print(f\"   • Final cache size: {cache_sizes[-1]}\")\n",
    "\n",
    "print(\"Visualizing memory evolution across segments...\\n\")\n",
    "visualize_memory_evolution(hmt_model, num_segments=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 6: Training Loop Execution\n",
    "\n",
    "## 6.1 Initialize Trainer\n",
    "\n",
    "Now we'll create the HMTTrainer and run the complete multi-stage training!\n",
    "\n",
    "**Paper Reference**: Algorithm 1 - HMT Training Procedure\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Create Trainer Instance\n",
    "\n",
    "# Create fresh HMT model for training\n",
    "hmt_model_train = HMT(backbone, hmt_config).to(device)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = HMTTrainer(\n",
    "    model=hmt_model_train,\n",
    "    config=training_config,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    ")\n",
    "\n",
    "print(\"✅ HMTTrainer Initialized!\")\n",
    "print(f\"   Device: {trainer.device}\")\n",
    "print(f\"   Optimizer: AdamW\")\n",
    "print(f\"   LR Scheduler: ExponentialDecay\")\n",
    "print(f\"   Gradient Monitor: Active\")\n",
    "print(f\"\\n📂 Checkpoints will be saved to: {training_config.output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Run Multi-Stage Training!\n",
    "\n",
    "print(\"🚀 Starting Multi-Stage Training!\")\n",
    "print(\"=\"*80)\n",
    "print(\"This will run:\")\n",
    "print(f\"  1. Stage 1: {training_config.stage1_steps} steps WITHOUT memory retrieval\")\n",
    "print(f\"  2. Stage 2: {training_config.stage2_steps} steps WITH memory retrieval\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n⏱️  Training will take a few minutes...\\n\")\n",
    "\n",
    "# Run training\n",
    "training_stats = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ Training Complete!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n📊 Final Statistics:\")\n",
    "print(f\"   Stage 1 Final PPL: {training_stats['stage1']['final_perplexity']:.2f}\")\n",
    "print(f\"   Stage 2 Final PPL: {training_stats['stage2']['final_perplexity']:.2f}\")\n",
    "print(f\"   Best Eval Metric: {training_stats['best_eval_metric']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 7: Checkpoint Management\n",
    "\n",
    "## 7.1 Understanding HMT Checkpoints\n",
    "\n",
    "**Paper Reference**: Appendix F - Stage 2 loads checkpoint from Stage 1\n",
    "\n",
    "Checkpoints contain:\n",
    "- Model state (all parameters)\n",
    "- Optimizer state (momentum, etc.)\n",
    "- Scheduler state (current step)\n",
    "- Training metadata (global step, current stage, best metric)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Inspect Saved Checkpoints\n",
    "\n",
    "import glob\n",
    "\n",
    "checkpoint_dir = Path(training_config.output_dir)\n",
    "checkpoints = sorted(checkpoint_dir.glob(\"*.pt\"))\n",
    "\n",
    "print(\"📂 Saved Checkpoints:\")\n",
    "print(\"=\"*80)\n",
    "for ckpt in checkpoints:\n",
    "    size_mb = ckpt.stat().st_size / (1024 * 1024)\n",
    "    print(f\"   {ckpt.name:<30} ({size_mb:.2f} MB)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n💡 Important Checkpoints:\")\n",
    "print(\"   • stage1_final.pt: End of Stage 1 (loaded by Stage 2)\")\n",
    "print(\"   • final.pt: End of Stage 2 (complete training)\")\n",
    "print(\"   • best_stageX.pt: Best model based on eval metric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Demonstrate Checkpoint Loading\n",
    "\n",
    "# Create fresh model\n",
    "hmt_fresh = HMT(backbone, hmt_config).to(device)\n",
    "\n",
    "# Create fresh trainer\n",
    "trainer_fresh = HMTTrainer(\n",
    "    model=hmt_fresh,\n",
    "    config=training_config,\n",
    "    train_dataloader=train_dataloader,\n",
    "    eval_dataloader=eval_dataloader,\n",
    ")\n",
    "\n",
    "print(\"🔄 Loading checkpoint: final.pt\\n\")\n",
    "\n",
    "# Load checkpoint\n",
    "final_checkpoint = checkpoint_dir / \"final.pt\"\n",
    "if final_checkpoint.exists():\n",
    "    trainer_fresh.load_checkpoint(final_checkpoint)\n",
    "    \n",
    "    print(\"✅ Checkpoint loaded successfully!\")\n",
    "    print(f\"   Global step: {trainer_fresh.global_step}\")\n",
    "    print(f\"   Current stage: {trainer_fresh.current_stage}\")\n",
    "    print(f\"   Best eval metric: {trainer_fresh.best_eval_metric:.4f}\")\n",
    "    print(\"\\n💡 Training can now be resumed from this point!\")\n",
    "else:\n",
    "    print(\"⚠️  Checkpoint not found. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 8: Ablation Studies\n",
    "\n",
    "## 8.1 Compare Training With vs Without Memory\n",
    "\n",
    "**Paper Reference**: Throughout the paper, memory retrieval is shown to improve performance.\n",
    "\n",
    "Let's compare:\n",
    "- **With Memory**: Full HMT (cross-attention retrieval)\n",
    "- **Without Memory**: Vanilla segmented processing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 Ablation: Effect of Memory Retrieval\n",
    "\n",
    "def compare_with_without_memory():\n",
    "    \"\"\"\n",
    "    Compare model outputs with and without memory retrieval.\n",
    "    \n",
    "    Paper: Memory retrieval improves long-context understanding\n",
    "    \"\"\"\n",
    "    # Load trained model\n",
    "    hmt_test = HMT(backbone, hmt_config).to(device)\n",
    "    hmt_test.eval()\n",
    "    \n",
    "    # Create long test sequence\n",
    "    test_seq_len = hmt_config.segment_length * 4  # 4 segments\n",
    "    test_input = torch.randint(0, 1000, (1, test_seq_len)).to(device)\n",
    "    \n",
    "    # Forward WITH memory\n",
    "    hmt_test.clear_memory()\n",
    "    with torch.no_grad():\n",
    "        outputs_with = hmt_test(test_input, use_memory=True)\n",
    "    stats_with = hmt_test.get_memory_stats()\n",
    "    \n",
    "    # Forward WITHOUT memory\n",
    "    hmt_test.clear_memory()\n",
    "    with torch.no_grad():\n",
    "        outputs_without = hmt_test(test_input, use_memory=False)\n",
    "    stats_without = hmt_test.get_memory_stats()\n",
    "    \n",
    "    # Compare\n",
    "    logits_with = outputs_with['logits']\n",
    "    logits_without = outputs_without['logits']\n",
    "    \n",
    "    # Compute difference\n",
    "    diff = torch.abs(logits_with - logits_without).mean().item()\n",
    "    \n",
    "    print(\"🔬 Ablation Study: Memory Retrieval Effect\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Test sequence length: {test_seq_len} tokens ({test_seq_len // hmt_config.segment_length} segments)\")\n",
    "    print()\n",
    "    print(\"WITH Memory:\")\n",
    "    print(f\"   Cache size: {stats_with['cache_size']}\")\n",
    "    print(f\"   Sensory memory: {'Active' if stats_with['sensory_memory_active'] else 'Inactive'}\")\n",
    "    print()\n",
    "    print(\"WITHOUT Memory:\")\n",
    "    print(f\"   Cache size: {stats_without['cache_size']}\")\n",
    "    print(f\"   Sensory memory: {'Active' if stats_without['sensory_memory_active'] else 'Inactive'}\")\n",
    "    print()\n",
    "    print(f\"Mean absolute difference in logits: {diff:.4f}\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n💡 Observations:\")\n",
    "    print(\"   • Memory retrieval changes predictions (non-zero difference)\")\n",
    "    print(\"   • WITH memory: model can access distant context\")\n",
    "    print(\"   • WITHOUT memory: only local context available\")\n",
    "\n",
    "compare_with_without_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🎓 Summary and Next Steps\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "1. **BPTT Mechanics**: \n",
    "   - Gradients flow backwards through K segments\n",
    "   - Stage 1 (K=2) vs Stage 2 (K=15) trade-offs\n",
    "   - Gradient monitoring for stability\n",
    "\n",
    "2. **Multi-Stage Training**:\n",
    "   - Stage 1: Learn segment encoding (faster)\n",
    "   - Stage 2: Learn memory retrieval (complete architecture)\n",
    "   - Checkpoint transfer between stages\n",
    "\n",
    "3. **Memory System**:\n",
    "   - Sensory memory (k=32): local continuity\n",
    "   - Short-term memory (L=256): current segment\n",
    "   - Long-term memory (N=300): distant context\n",
    "   - FIFO cache management\n",
    "\n",
    "4. **Gradient Stability**:\n",
    "   - Gradient monitoring and clipping\n",
    "   - HMT is more stable than RMT (Appendix J)\n",
    "   - No vanishing/explosion issues\n",
    "\n",
    "## Next Steps for Real Training\n",
    "\n",
    "1. **Use Real Dataset**:\n",
    "   ```bash\n",
    "   python experiments/scripts/prepare_wikitext.py\n",
    "   ```\n",
    "\n",
    "2. **Scale Up Training**:\n",
    "   - Increase `stage1_steps` to 200\n",
    "   - Increase `stage2_steps` to 500  \n",
    "   - Increase `stage2_bptt_segments` to 15\n",
    "   - Use larger `num_memory_embeddings` (N=300)\n",
    "\n",
    "3. **Try Different Backbones**:\n",
    "   - OPT-350M (use `get_opt_350m_config()`)\n",
    "   - OPT-2.7B (use `get_opt_2_7b_config()`)\n",
    "   - LLaMA-2-7B (use `get_llama2_7b_config()` with LoRA)\n",
    "\n",
    "4. **Enable W&B Logging**:\n",
    "   ```python\n",
    "   training_config.use_wandb = True\n",
    "   training_config.wandb_project = \"hmt-training\"\n",
    "   ```\n",
    "\n",
    "5. **Evaluate on Long-Context Tasks**:\n",
    "   - Perplexity on increasing context lengths\n",
    "   - Compare vs vanilla transformer\n",
    "   - Measure memory efficiency\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Additional Resources\n",
    "\n",
    "- **Paper**: `HMT-paper.pdf` in project root\n",
    "- **Tests**: `tests/test_training.py` for comprehensive testing\n",
    "- **Training Script**: `experiments/scripts/train_hmt.py` for production\n",
    "- **Documentation**: `README.md` and `CLAUDE.md`\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Congratulations on completing the interactive HMT training tutorial!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
