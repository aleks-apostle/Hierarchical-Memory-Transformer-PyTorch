{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete HMT Pipeline: Understanding the Three-Level Memory Hierarchy\n",
    "\n",
    "**Paper Reference:** [Hierarchical Memory Transformer for Efficient Long Context Language Processing](https://arxiv.org/abs/2405.06067)\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand how **MemoryEmbeddingGenerator** creates compressed memory representations\n",
    "2. See the complete **HMT forward pass** in action with all components integrated\n",
    "3. Visualize the **three-level memory hierarchy**: sensory, short-term, and long-term\n",
    "4. Process long sequences and observe memory cache evolution\n",
    "5. Compare HMT vs standard transformer behavior\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from hmt import HMT, HMTConfig\n",
    "from hmt.memory import MemoryEmbeddingGenerator\n",
    "from hmt.utils import get_device\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Get device\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Device type: {device.type if hasattr(device, 'type') else device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Memory Embedding Generation\n",
    "\n",
    "**Paper Section 3.3:** Memory embeddings are compressed representations of processed segments.\n",
    "\n",
    "### Equation 4: $m_n = \\text{compress}(\\text{BBM}([k_n || H_n || P_n]))$\n",
    "\n",
    "Where:\n",
    "- $k_n$: Sensory memory (last k tokens from previous segment)\n",
    "- $H_n$: Current segment (L tokens)\n",
    "- $P_n$: Retrieved memory from cache\n",
    "- $m_n$: Generated memory embedding to store in cache\n",
    "\n",
    "### Understanding Different Extraction Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a small model for demonstration\n",
    "print(\"Loading GPT-2...\")\n",
    "backbone = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "backbone.eval()\n",
    "\n",
    "# Create config\n",
    "config = HMTConfig(\n",
    "    segment_length=128,\n",
    "    representation_length=64,\n",
    "    num_memory_embeddings=50,\n",
    "    sensory_memory_size=16,\n",
    "    hidden_dim=768,  # GPT-2 hidden size\n",
    ")\n",
    "\n",
    "print(f\"\\nHMT Configuration:\")\n",
    "print(f\"  Segment length (L): {config.segment_length}\")\n",
    "print(f\"  Representation length (j): {config.representation_length}\")\n",
    "print(f\"  Memory cache size (N): {config.num_memory_embeddings}\")\n",
    "print(f\"  Sensory memory size (k): {config.sensory_memory_size}\")\n",
    "print(f\"  Hidden dimension: {config.hidden_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MemoryEmbeddingGenerator\n",
    "mem_gen = MemoryEmbeddingGenerator(config).to(device)\n",
    "\n",
    "# Simulate backbone output (hidden states)\n",
    "batch_size = 1\n",
    "seq_len = 100\n",
    "simulated_hidden_states = torch.randn(batch_size, seq_len, config.hidden_dim).to(device)\n",
    "\n",
    "print(\"Testing different extraction strategies:\\n\")\n",
    "\n",
    "strategies = [\"last\", \"mean\", \"max\", \"cls\"]\n",
    "strategy_embeddings = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for strategy in strategies:\n",
    "        embedding = mem_gen(simulated_hidden_states, extraction_strategy=strategy)\n",
    "        strategy_embeddings[strategy] = embedding\n",
    "        print(f\"  {strategy:8s} ‚Üí shape: {embedding.shape}, mean: {embedding.mean():.4f}, std: {embedding.std():.4f}\")\n",
    "\n",
    "print(\"\\nüìö Strategy Explanations:\")\n",
    "print(\"  ‚Ä¢ 'last':  Uses final token (best for causal LMs like GPT)\")\n",
    "print(\"  ‚Ä¢ 'mean':  Average over all tokens (balanced representation)\")\n",
    "print(\"  ‚Ä¢ 'max':   Max pool (captures salient features)\")\n",
    "print(\"  ‚Ä¢ 'cls':   First token (BERT-style, less common for GPT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the differences between strategies\n",
    "fig, axes = plt.subplots(1, len(strategies), figsize=(16, 4))\n",
    "\n",
    "for idx, (strategy, embedding) in enumerate(strategy_embeddings.items()):\n",
    "    emb_np = embedding.cpu().numpy().flatten()[:100]  # Show first 100 dimensions\n",
    "    axes[idx].bar(range(len(emb_np)), emb_np, alpha=0.7)\n",
    "    axes[idx].set_title(f\"{strategy.capitalize()} Strategy\", fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel(\"Dimension\")\n",
    "    axes[idx].set_ylabel(\"Value\")\n",
    "    axes[idx].axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Memory Embedding Patterns Across Different Extraction Strategies\", \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Observation: Different strategies produce different embedding patterns.\")\n",
    "print(\"   The 'last' strategy (paper's approach) captures final state after full context processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Complete HMT Forward Pass\n",
    "\n",
    "**Paper Algorithm 1:** Processing long sequences with hierarchical memory\n",
    "\n",
    "### The Three-Level Memory Hierarchy\n",
    "\n",
    "1. **Sensory Memory (k=16 tokens)**: \n",
    "   - Preserves last 16 tokens from previous segment\n",
    "   - Provides local continuity across segments\n",
    "   - Sliding window that moves with each segment\n",
    "\n",
    "2. **Short-term Memory (L=128 tokens)**:\n",
    "   - Current segment being processed\n",
    "   - Standard transformer attention within this window\n",
    "   - Processes current context\n",
    "\n",
    "3. **Long-term Memory (N=50 embeddings)**:\n",
    "   - Cache of compressed memory embeddings from past segments\n",
    "   - Retrieved via cross-attention when relevant\n",
    "   - Enables access to distant context without O(L¬≤) complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HMT\n",
    "hmt = HMT(backbone, config).to(device)\n",
    "hmt.eval()\n",
    "\n",
    "print(\"HMT Model Initialized!\")\n",
    "print(f\"\\nMemory Components:\")\n",
    "print(f\"  ‚úì RepresentationEncoder\")\n",
    "print(f\"  ‚úì MemorySearch\")\n",
    "print(f\"  ‚úì MemoryEmbeddingGenerator\")\n",
    "print(f\"  ‚úì Memory Cache (FIFO queue, max size: {config.num_memory_embeddings})\")\n",
    "print(f\"  ‚úì Sensory Memory (last {config.sensory_memory_size} tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step: Processing a Long Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample long text (from WikiText-like article)\n",
    "long_text = \"\"\"\n",
    "The Hierarchical Memory Transformer is a novel architecture designed to handle long-context \n",
    "language processing efficiently. Traditional transformers face quadratic complexity with sequence \n",
    "length, making them impractical for very long documents. HMT addresses this by introducing a \n",
    "three-level memory hierarchy inspired by human cognition.\n",
    "\n",
    "The first level, sensory memory, preserves the most recent tokens from the previous segment, \n",
    "ensuring local continuity. The second level, short-term memory, processes the current segment \n",
    "of fixed length L. The third level, long-term memory, maintains a cache of compressed embeddings \n",
    "from all previous segments, allowing the model to retrieve relevant distant context when needed.\n",
    "\n",
    "This design reduces computational complexity from O(L¬≤) to O(L) per segment, while still enabling \n",
    "access to the entire context through the memory retrieval mechanism. Cross-attention is used to \n",
    "search the long-term cache for relevant memories based on the current segment's representation.\n",
    "\n",
    "The memory embeddings are generated by processing augmented segments through the backbone model \n",
    "and extracting compressed representations. These embeddings capture the essential information \n",
    "from each segment in a fixed-size vector, which is then stored in the cache for future retrieval.\n",
    "\n",
    "Experimental results show that HMT achieves competitive performance on long-context language \n",
    "modeling tasks while using significantly less memory and computation than standard transformers. \n",
    "The architecture is plug-and-play, working with any pre-trained decoder-only transformer without \n",
    "modification to the backbone model.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(long_text, return_tensors=\"pt\", truncation=False)\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "seq_length = input_ids.shape[1]\n",
    "num_segments = (seq_length + config.segment_length - 1) // config.segment_length\n",
    "\n",
    "print(f\"Input Text Statistics:\")\n",
    "print(f\"  Total tokens: {seq_length}\")\n",
    "print(f\"  Number of segments: {num_segments}\")\n",
    "print(f\"  Tokens per segment: {config.segment_length}\")\n",
    "print(f\"  Last segment size: {seq_length % config.segment_length if seq_length % config.segment_length != 0 else config.segment_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear memory before processing\n",
    "hmt.clear_memory()\n",
    "print(\"Memory cleared. Starting fresh...\\n\")\n",
    "\n",
    "# Track memory evolution\n",
    "memory_stats_timeline = []\n",
    "\n",
    "# Process with HMT\n",
    "with torch.no_grad():\n",
    "    outputs = hmt(input_ids, attention_mask=attention_mask, use_memory=True)\n",
    "\n",
    "# Get final memory stats\n",
    "final_stats = hmt.get_memory_stats()\n",
    "\n",
    "print(\"‚úÖ Processing complete!\\n\")\n",
    "print(f\"Final Memory State:\")\n",
    "print(f\"  Cache size: {final_stats['cache_size']} / {final_stats['max_cache_size']}\")\n",
    "print(f\"  Sensory memory active: {final_stats['sensory_memory_active']}\")\n",
    "print(f\"  Sensory memory size: {final_stats['sensory_memory_size']} tokens\")\n",
    "\n",
    "print(f\"\\nOutput shape: {outputs['logits'].shape}\")\n",
    "print(f\"Expected: [batch_size=1, seq_len={seq_length}, vocab_size=50257]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Memory Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process again, tracking cache growth at each segment\n",
    "hmt.clear_memory()\n",
    "cache_sizes = []\n",
    "\n",
    "# Manually segment and track\n",
    "input_embeddings = hmt.embedding_layer(input_ids)\n",
    "\n",
    "for seg_idx in range(num_segments):\n",
    "    start = seg_idx * config.segment_length\n",
    "    end = min(start + config.segment_length, seq_length)\n",
    "    \n",
    "    # Process segment (simplified - just track cache growth)\n",
    "    segment_ids = input_ids[:, start:end]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = hmt(segment_ids, use_memory=True)\n",
    "    \n",
    "    cache_sizes.append(len(hmt.memory_cache))\n",
    "\n",
    "# Plot cache growth\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_segments + 1), cache_sizes, marker='o', linewidth=2, markersize=8)\n",
    "plt.axhline(y=config.num_memory_embeddings, color='r', linestyle='--', \n",
    "            label=f'Max Cache Size (N={config.num_memory_embeddings})')\n",
    "plt.xlabel('Segment Number', fontsize=12)\n",
    "plt.ylabel('Cache Size', fontsize=12)\n",
    "plt.title('Long-term Memory Cache Growth Over Segments', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "segment_positions = np.arange(num_segments) * config.segment_length\n",
    "segment_widths = [config.segment_length] * (num_segments - 1) + [seq_length % config.segment_length or config.segment_length]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_segments))\n",
    "\n",
    "for i, (pos, width) in enumerate(zip(segment_positions, segment_widths)):\n",
    "    plt.barh(0, width, left=pos, height=0.5, color=colors[i], \n",
    "             label=f'Seg {i+1}' if i < 5 or i == num_segments-1 else None)\n",
    "\n",
    "plt.xlabel('Token Position', fontsize=12)\n",
    "plt.title('Sequence Segmentation', fontsize=14, fontweight='bold')\n",
    "plt.yticks([])\n",
    "plt.legend(loc='upper right', ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Insights:\")\n",
    "print(f\"  ‚Ä¢ Cache grows linearly with segments (FIFO queue)\")\n",
    "print(f\"  ‚Ä¢ Each segment adds one memory embedding to cache\")\n",
    "print(f\"  ‚Ä¢ Sequence is divided into {num_segments} segments for processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Ablation Study - With vs Without Memory\n",
    "\n",
    "**Key Question:** How does memory retrieval affect HMT's processing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test shorter text for clearer comparison\n",
    "test_text = \"The transformer architecture revolutionized natural language processing by introducing \"\n",
    "test_text += \"self-attention mechanisms. However, long sequences remain computationally expensive. \"\n",
    "test_text += \"Hierarchical memory transformers address this challenge efficiently.\"\n",
    "\n",
    "test_inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=False)\n",
    "test_ids = test_inputs[\"input_ids\"].to(device)\n",
    "\n",
    "print(f\"Test sequence length: {test_ids.shape[1]} tokens\\n\")\n",
    "\n",
    "# Process WITH memory\n",
    "hmt.clear_memory()\n",
    "with torch.no_grad():\n",
    "    outputs_with_mem = hmt(test_ids, use_memory=True)\n",
    "\n",
    "stats_with = hmt.get_memory_stats()\n",
    "\n",
    "# Process WITHOUT memory\n",
    "hmt.clear_memory()\n",
    "with torch.no_grad():\n",
    "    outputs_without_mem = hmt(test_ids, use_memory=False)\n",
    "\n",
    "stats_without = hmt.get_memory_stats()\n",
    "\n",
    "print(\"\\nüìä Comparison:\")\n",
    "print(f\"\\nWith Memory:\")\n",
    "print(f\"  Cache size: {stats_with['cache_size']}\")\n",
    "print(f\"  Sensory memory: {'Active' if stats_with['sensory_memory_active'] else 'Inactive'}\")\n",
    "print(f\"  Output shape: {outputs_with_mem['logits'].shape}\")\n",
    "\n",
    "print(f\"\\nWithout Memory (Ablation):\")\n",
    "print(f\"  Cache size: {stats_without['cache_size']}\")\n",
    "print(f\"  Sensory memory: {'Active' if stats_without['sensory_memory_active'] else 'Inactive'}\")\n",
    "print(f\"  Output shape: {outputs_without_mem['logits'].shape}\")\n",
    "\n",
    "# Check if outputs differ\n",
    "output_diff = (outputs_with_mem['logits'] - outputs_without_mem['logits']).abs().mean().item()\n",
    "print(f\"\\nMean absolute difference in logits: {output_diff:.6f}\")\n",
    "print(f\"Outputs are {'identical' if output_diff < 1e-6 else 'different'} (memory {'does not affect' if output_diff < 1e-6 else 'affects'} processing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Understanding the Augmented Input\n",
    "\n",
    "**Paper Section 3.3:** Each segment is augmented with:\n",
    "- Sensory memory (k tokens from previous segment)\n",
    "- Current segment (L tokens)\n",
    "- Retrieved memory embedding (1 pseudo-token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmented input structure\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Example for segment 3 (has sensory memory)\n",
    "sensory_size = config.sensory_memory_size\n",
    "segment_size = config.segment_length\n",
    "memory_token = 1\n",
    "\n",
    "total_augmented = sensory_size + segment_size + memory_token\n",
    "\n",
    "# Plot boxes\n",
    "ax.barh(0, sensory_size, left=0, height=0.6, \n",
    "        color='lightblue', edgecolor='black', linewidth=2, label='Sensory Memory (k=16)')\n",
    "ax.barh(0, segment_size, left=sensory_size, height=0.6, \n",
    "        color='lightgreen', edgecolor='black', linewidth=2, label='Current Segment (L=128)')\n",
    "ax.barh(0, memory_token, left=sensory_size + segment_size, height=0.6, \n",
    "        color='coral', edgecolor='black', linewidth=2, label='Retrieved Memory (1 token)')\n",
    "\n",
    "# Annotations\n",
    "ax.text(sensory_size/2, 0, f'{sensory_size}', ha='center', va='center', \n",
    "        fontsize=12, fontweight='bold')\n",
    "ax.text(sensory_size + segment_size/2, 0, f'{segment_size}', ha='center', va='center', \n",
    "        fontsize=12, fontweight='bold')\n",
    "ax.text(sensory_size + segment_size + memory_token/2, 0, '1', ha='center', va='center', \n",
    "        fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xlim(-5, total_augmented + 5)\n",
    "ax.set_ylim(-0.5, 0.5)\n",
    "ax.set_xlabel('Token Position', fontsize=12)\n",
    "ax.set_title('Augmented Input Structure for Segment Processing', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=11)\n",
    "ax.set_yticks([])\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "# Add arrows and labels\n",
    "ax.annotate('', xy=(0, -0.35), xytext=(sensory_size, -0.35),\n",
    "            arrowprops=dict(arrowstyle='<->', color='blue', lw=2))\n",
    "ax.text(sensory_size/2, -0.45, 'Last k tokens\\nfrom prev segment', \n",
    "        ha='center', va='top', fontsize=9, color='blue')\n",
    "\n",
    "ax.annotate('', xy=(sensory_size, -0.35), xytext=(sensory_size + segment_size, -0.35),\n",
    "            arrowprops=dict(arrowstyle='<->', color='green', lw=2))\n",
    "ax.text(sensory_size + segment_size/2, -0.45, 'New input tokens', \n",
    "        ha='center', va='top', fontsize=9, color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìù Augmented Input Components:\")\n",
    "print(f\"  1. Sensory Memory: {sensory_size} tokens (local continuity)\")\n",
    "print(f\"  2. Current Segment: {segment_size} tokens (new input)\")\n",
    "print(f\"  3. Retrieved Memory: {memory_token} pseudo-token (distant context)\")\n",
    "print(f\"  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "print(f\"  Total augmented length: {total_augmented} tokens\")\n",
    "print(f\"\\n  The backbone processes this augmented input to generate outputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Hands-On Exercises\n",
    "\n",
    "### Exercise 1: Experiment with Different Segment Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Try different segment_length values (64, 128, 256)\n",
    "# Observe how it affects number of segments and cache size\n",
    "\n",
    "segment_lengths = [64, 128, 256]\n",
    "results = []\n",
    "\n",
    "sample_text = long_text  # Use the long text from earlier\n",
    "sample_inputs = tokenizer(sample_text, return_tensors=\"pt\", truncation=False)\n",
    "sample_ids = sample_inputs[\"input_ids\"].to(device)\n",
    "total_tokens = sample_ids.shape[1]\n",
    "\n",
    "print(f\"Sample text length: {total_tokens} tokens\\n\")\n",
    "\n",
    "for seg_len in segment_lengths:\n",
    "    # Create config with different segment length\n",
    "    test_config = HMTConfig(\n",
    "        segment_length=seg_len,\n",
    "        representation_length=seg_len // 2,\n",
    "        num_memory_embeddings=50,\n",
    "        sensory_memory_size=16,\n",
    "        hidden_dim=768,\n",
    "    )\n",
    "    \n",
    "    # Create HMT\n",
    "    test_hmt = HMT(backbone, test_config).to(device)\n",
    "    test_hmt.eval()\n",
    "    \n",
    "    # Process\n",
    "    with torch.no_grad():\n",
    "        _ = test_hmt(sample_ids, use_memory=True)\n",
    "    \n",
    "    stats = test_hmt.get_memory_stats()\n",
    "    num_segs = (total_tokens + seg_len - 1) // seg_len\n",
    "    \n",
    "    results.append({\n",
    "        'segment_length': seg_len,\n",
    "        'num_segments': num_segs,\n",
    "        'cache_size': stats['cache_size']\n",
    "    })\n",
    "    \n",
    "    print(f\"Segment Length = {seg_len}:\")\n",
    "    print(f\"  Number of segments: {num_segs}\")\n",
    "    print(f\"  Final cache size: {stats['cache_size']}\")\n",
    "    print()\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "seg_lens = [r['segment_length'] for r in results]\n",
    "num_segs = [r['num_segments'] for r in results]\n",
    "cache_sizes = [r['cache_size'] for r in results]\n",
    "\n",
    "ax1.bar(range(len(seg_lens)), num_segs, tick_label=seg_lens, color='steelblue')\n",
    "ax1.set_xlabel('Segment Length', fontsize=12)\n",
    "ax1.set_ylabel('Number of Segments', fontsize=12)\n",
    "ax1.set_title('Segments vs Segment Length', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "ax2.bar(range(len(seg_lens)), cache_sizes, tick_label=seg_lens, color='coral')\n",
    "ax2.set_xlabel('Segment Length', fontsize=12)\n",
    "ax2.set_ylabel('Final Cache Size', fontsize=12)\n",
    "ax2.set_title('Cache Size vs Segment Length', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç Observation: Smaller segments ‚Üí More segments ‚Üí Larger cache (up to max N)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Analyze Memory Cache Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test what happens when cache exceeds max size N\n",
    "# Create a very long sequence that generates more than N memory embeddings\n",
    "\n",
    "# Your code here:\n",
    "# 1. Create a config with small N (e.g., N=5)\n",
    "# 2. Create a long sequence that will generate 10+ segments\n",
    "# 3. Process and observe cache behavior (FIFO eviction)\n",
    "\n",
    "# Example solution:\n",
    "small_cache_config = HMTConfig(\n",
    "    segment_length=32,\n",
    "    num_memory_embeddings=5,  # Small cache\n",
    "    sensory_memory_size=8,\n",
    "    hidden_dim=768,\n",
    ")\n",
    "\n",
    "small_cache_hmt = HMT(backbone, small_cache_config).to(device)\n",
    "small_cache_hmt.eval()\n",
    "\n",
    "# Long input (should create ~10 segments with seg_len=32)\n",
    "long_input = torch.randint(0, 1000, (1, 320)).to(device)  # 320 tokens = 10 segments\n",
    "\n",
    "# Track cache size after each segment\n",
    "cache_evolution = []\n",
    "for i in range(0, 320, 32):\n",
    "    chunk = long_input[:, i:i+32]\n",
    "    with torch.no_grad():\n",
    "        _ = small_cache_hmt(chunk, use_memory=True)\n",
    "    cache_evolution.append(len(small_cache_hmt.memory_cache))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(cache_evolution)+1), cache_evolution, marker='o', linewidth=2)\n",
    "plt.axhline(y=5, color='r', linestyle='--', linewidth=2, label='Max Cache Size (N=5)')\n",
    "plt.xlabel('Segment Number', fontsize=12)\n",
    "plt.ylabel('Cache Size', fontsize=12)\n",
    "plt.title('FIFO Cache Behavior: Eviction When Exceeding Max Size', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Cache Evolution: {cache_evolution}\")\n",
    "print(f\"\\nüìö Insight: Cache grows to N={small_cache_config.num_memory_embeddings}, then oldest memories are evicted (FIFO).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Compare Computational Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze complexity difference between standard transformer and HMT\n",
    "\n",
    "sequence_lengths = [128, 256, 512, 1024, 2048]\n",
    "segment_length = 128\n",
    "\n",
    "standard_complexity = []  # O(L¬≤)\n",
    "hmt_complexity = []       # O(L) per segment\n",
    "\n",
    "for L in sequence_lengths:\n",
    "    # Standard transformer: O(L¬≤) attention\n",
    "    standard_ops = L * L\n",
    "    standard_complexity.append(standard_ops)\n",
    "    \n",
    "    # HMT: O(segment_length) per segment + O(N) memory retrieval\n",
    "    num_segments = (L + segment_length - 1) // segment_length\n",
    "    hmt_ops = num_segments * segment_length * segment_length  # Attention within segments\n",
    "    hmt_ops += num_segments * config.num_memory_embeddings    # Memory retrieval\n",
    "    hmt_complexity.append(hmt_ops)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(sequence_lengths, standard_complexity, marker='s', linewidth=3, \n",
    "         label='Standard Transformer O(L¬≤)', color='red')\n",
    "plt.plot(sequence_lengths, hmt_complexity, marker='o', linewidth=3, \n",
    "         label=f'HMT O(L) per segment (seg_len={segment_length})', color='green')\n",
    "\n",
    "plt.xlabel('Sequence Length (L)', fontsize=12)\n",
    "plt.ylabel('Approximate Operations', fontsize=12)\n",
    "plt.title('Computational Complexity: Standard Transformer vs HMT', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')  # Log scale for better visualization\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate speedup\n",
    "speedups = [s / h for s, h in zip(standard_complexity, hmt_complexity)]\n",
    "print(\"\\nüìä Speedup (Standard / HMT):\")\n",
    "for L, speedup in zip(sequence_lengths, speedups):\n",
    "    print(f\"  L={L:5d}: {speedup:.2f}x faster\")\n",
    "\n",
    "print(\"\\nüí° Key Insight: HMT's complexity scales much better for long sequences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Memory Embedding Generation** (Section 3.3):\n",
    "   - Different extraction strategies: last, mean, max, cls\n",
    "   - Compression layer learns optimal memory representation\n",
    "   - Memory embeddings are fixed-size regardless of segment length\n",
    "\n",
    "2. **Complete HMT Pipeline**:\n",
    "   - Segmentation of long sequences into manageable chunks\n",
    "   - Three-level memory hierarchy working together:\n",
    "     * Sensory (local continuity)\n",
    "     * Short-term (current processing)\n",
    "     * Long-term (distant context)\n",
    "   - FIFO cache management for bounded memory usage\n",
    "\n",
    "3. **Computational Efficiency**:\n",
    "   - HMT reduces O(L¬≤) to O(L) per segment\n",
    "   - Scales much better for very long sequences\n",
    "   - Memory retrieval adds only O(N) operations\n",
    "\n",
    "4. **Design Insights**:\n",
    "   - Ablation study shows memory affects processing\n",
    "   - Augmented input combines: sensory + segment + retrieved memory\n",
    "   - Cache size impacts available long-term context\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- **Phase 4**: Implement training with BPTT (Backpropagation Through Time)\n",
    "- **Phase 5**: Fine-tune HMT on long-context tasks\n",
    "- **Phase 6**: Evaluate on WikiText-103 and other benchmarks\n",
    "- **Phase 7**: Scale to larger models (LLaMA, OPT-350M+)\n",
    "\n",
    "---\n",
    "\n",
    "### üéì Congratulations!\n",
    "\n",
    "You now understand how the complete HMT system processes long sequences efficiently using hierarchical memory. The three-level architecture enables the model to maintain both local and distant context while keeping computational complexity linear.\n",
    "\n",
    "**Paper:** [arXiv:2405.06067](https://arxiv.org/abs/2405.06067)  \n",
    "**Implementation:** HMT-implementation (Phase 3.2 Complete)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
