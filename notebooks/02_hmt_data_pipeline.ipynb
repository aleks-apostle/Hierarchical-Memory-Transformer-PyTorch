{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMT Data Pipeline: Hands-On Exploration\n",
    "\n",
    "Now that you understand WikiText-103, let's use the **HMT data pipeline** you just built!\n",
    "\n",
    "**In this notebook:**\n",
    "1. Load data using your `WikiTextDataset` and `LongContextDataLoader`\n",
    "2. Inspect batches and understand padding\n",
    "3. Simulate how HMT will process long articles\n",
    "4. Compare standard transformer truncation vs HMT segmentation\n",
    "5. Prepare for implementing HMT memory components\n",
    "\n",
    "**Learning Goals:**\n",
    "- Understand the data flow from raw text â†’ tokens â†’ batches â†’ HMT segments\n",
    "- See why HMT's approach is superior for long contexts\n",
    "- Get intuition for the three-level memory hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hmt.data import WikiTextDataset, LongContextDataLoader, create_dataloaders\n",
    "from hmt.utils import get_device\n",
    "from hmt.config import HMTConfig\n",
    "\n",
    "# Check device\n",
    "device = get_device()\n",
    "print(f\"ðŸš€ Using device: {device}\")\n",
    "print(f\"   MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data with HMT Pipeline\n",
    "\n",
    "Let's use the data utilities you built in `src/hmt/data.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer: GPT-2\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Pad token: '{tokenizer.pad_token}' (id: {tokenizer.pad_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders using your utility function\n",
    "print(\"Creating dataloaders...\\n\")\n",
    "\n",
    "dataloaders = create_dataloaders(\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=4,\n",
    "    min_length=128,\n",
    "    max_length=4096,  # Limit for notebook speed\n",
    ")\n",
    "\n",
    "train_loader = dataloaders['train']\n",
    "val_loader = dataloaders['validation']\n",
    "test_loader = dataloaders['test']\n",
    "\n",
    "print(f\"âœ… Dataloaders created:\")\n",
    "print(f\"  Train:      {len(train_loader)} batches\")\n",
    "print(f\"  Validation: {len(val_loader)} batches\")\n",
    "print(f\"  Test:       {len(test_loader)} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inspecting a Batch\n",
    "\n",
    "Let's look at what a batch looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch from validation set\n",
    "batch = next(iter(val_loader))\n",
    "\n",
    "print(\"ðŸ“¦ Batch structure:\")\n",
    "print(f\"  Keys: {list(batch.keys())}\")\n",
    "print(f\"\\n  input_ids shape: {batch['input_ids'].shape}\")\n",
    "print(f\"  attention_mask shape: {batch['attention_mask'].shape}\")\n",
    "\n",
    "batch_size, seq_len = batch['input_ids'].shape\n",
    "print(f\"\\n  Batch size: {batch_size} articles\")\n",
    "print(f\"  Sequence length: {seq_len} tokens (padded to longest in batch)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each article in the batch\n",
    "print(\"\\nðŸ“Š Article lengths in this batch:\\n\")\n",
    "\n",
    "for i in range(batch_size):\n",
    "    # Count non-padded tokens using attention mask\n",
    "    actual_length = batch['attention_mask'][i].sum().item()\n",
    "    padding = seq_len - actual_length\n",
    "    \n",
    "    print(f\"  Article {i}:\")\n",
    "    print(f\"    Actual tokens: {actual_length}\")\n",
    "    print(f\"    Padding:       {padding}\")\n",
    "    print(f\"    Efficiency:    {actual_length/seq_len*100:.1f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode and display the first article\n",
    "article_0 = batch['input_ids'][0]\n",
    "article_0_length = batch['attention_mask'][0].sum().item()\n",
    "\n",
    "# Remove padding\n",
    "article_0_clean = article_0[:article_0_length]\n",
    "\n",
    "# Decode\n",
    "text = tokenizer.decode(article_0_clean)\n",
    "\n",
    "print(\"ðŸ“– First article in batch:\")\n",
    "print(\"=\"*80)\n",
    "print(text[:500])\n",
    "print(\"...\")\n",
    "print(f\"\\nTotal length: {article_0_length} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simulating HMT Segmentation\n",
    "\n",
    "Now let's see how HMT would process this article with its segmentation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMT configuration\n",
    "config = HMTConfig(\n",
    "    segment_length=512,\n",
    "    num_memory_embeddings=300,\n",
    "    sensory_memory_size=32,\n",
    ")\n",
    "\n",
    "print(\"âš™ï¸  HMT Configuration:\")\n",
    "print(f\"  Segment length (L):          {config.segment_length} tokens\")\n",
    "print(f\"  Representation length (j):   {config.representation_length} tokens (L/2)\")\n",
    "print(f\"  Sensory memory size (k):     {config.sensory_memory_size} tokens\")\n",
    "print(f\"  Long-term memory cache (N):  {config.num_memory_embeddings} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_hmt_processing(tokens, config):\n",
    "    \"\"\"\n",
    "    Visualize how HMT processes a long sequence.\n",
    "    \n",
    "    Returns information about each segment and the processing flow.\n",
    "    \"\"\"\n",
    "    L = config.segment_length\n",
    "    k = config.sensory_memory_size\n",
    "    j = config.representation_length\n",
    "    \n",
    "    num_tokens = len(tokens)\n",
    "    num_segments = (num_tokens + L - 1) // L\n",
    "    \n",
    "    print(f\"\\nðŸ”„ HMT Processing Pipeline:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total tokens: {num_tokens}\")\n",
    "    print(f\"Number of segments: {num_segments}\")\n",
    "    print(f\"\\nProcessing flow:\\n\")\n",
    "    \n",
    "    segments_info = []\n",
    "    \n",
    "    for seg_id in range(num_segments):\n",
    "        # Current segment bounds\n",
    "        start = seg_id * L\n",
    "        end = min(start + L, num_tokens)\n",
    "        seg_len = end - start\n",
    "        \n",
    "        # Sensory memory from previous segment\n",
    "        if seg_id > 0:\n",
    "            sensory_start = max(0, start - k)\n",
    "            sensory_len = start - sensory_start\n",
    "        else:\n",
    "            sensory_len = 0\n",
    "        \n",
    "        print(f\"ðŸ“ Segment {seg_id}:\")\n",
    "        print(f\"  Position: tokens {start:4d} - {end:4d}\")\n",
    "        print(f\"  Current segment:  {seg_len:3d} tokens\")\n",
    "        \n",
    "        if seg_id > 0:\n",
    "            print(f\"  Sensory memory:   {sensory_len:3d} tokens (from segment {seg_id-1})\")\n",
    "            print(f\"  Memory retrieval: Query {seg_id} past memory embeddings\")\n",
    "            print(f\"  Total context:    {seg_len + sensory_len:3d} tokens + retrieved memories\")\n",
    "        else:\n",
    "            print(f\"  Sensory memory:   None (first segment)\")\n",
    "            print(f\"  Memory retrieval: None (first segment)\")\n",
    "            print(f\"  Total context:    {seg_len:3d} tokens\")\n",
    "        \n",
    "        print(f\"  Encoding:         First {j} tokens â†’ representation embedding\")\n",
    "        print(f\"  Output:           New memory embedding â†’ cache\")\n",
    "        print()\n",
    "        \n",
    "        segments_info.append({\n",
    "            'id': seg_id,\n",
    "            'start': start,\n",
    "            'end': end,\n",
    "            'length': seg_len,\n",
    "            'sensory': sensory_len,\n",
    "        })\n",
    "    \n",
    "    return segments_info\n",
    "\n",
    "# Visualize processing for our article\n",
    "segments_info = visualize_hmt_processing(article_0_clean, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing Approaches: Standard Transformer vs HMT\n",
    "\n",
    "Let's visualize the key difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare coverage\n",
    "article_len = len(article_0_clean)\n",
    "gpt2_limit = 1024\n",
    "\n",
    "print(\"\\nâš”ï¸  Standard Transformer vs HMT Comparison:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nArticle length: {article_len} tokens\\n\")\n",
    "\n",
    "# Standard GPT-2\n",
    "print(\"âŒ Standard GPT-2:\")\n",
    "if article_len > gpt2_limit:\n",
    "    tokens_lost = article_len - gpt2_limit\n",
    "    pct_lost = (tokens_lost / article_len) * 100\n",
    "    print(f\"  Processes:  {gpt2_limit:4d} tokens\")\n",
    "    print(f\"  TRUNCATES:  {tokens_lost:4d} tokens ({pct_lost:.1f}%)\")\n",
    "    print(f\"  Memory:     O(LÂ²) = O({gpt2_limit}Â²) = {gpt2_limit**2:,} operations\")\n",
    "    print(f\"  âš ï¸  Context window slides - loses information!\")\n",
    "else:\n",
    "    print(f\"  Processes:  {article_len:4d} tokens (fits in context)\")\n",
    "    print(f\"  Memory:     O(LÂ²) = O({article_len}Â²) = {article_len**2:,} operations\")\n",
    "\n",
    "# HMT\n",
    "print(f\"\\nâœ… HMT with Hierarchical Memory:\")\n",
    "num_segments = len(segments_info)\n",
    "total_processed = sum(seg['length'] for seg in segments_info)\n",
    "print(f\"  Processes:  {total_processed:4d} tokens (FULL article)\")\n",
    "print(f\"  Segments:   {num_segments} segments of {config.segment_length} tokens\")\n",
    "print(f\"  Memory:     O(L) per segment = {num_segments} Ã— {config.segment_length} = {num_segments * config.segment_length:,} operations\")\n",
    "print(f\"  Cache:      {config.num_memory_embeddings} memory embeddings\")\n",
    "print(f\"  âœ… Hierarchical memory - preserves ALL context!\")\n",
    "\n",
    "# Efficiency gain\n",
    "if article_len > gpt2_limit:\n",
    "    print(f\"\\nðŸš€ Efficiency Gain:\")\n",
    "    standard_ops = gpt2_limit ** 2\n",
    "    hmt_ops = num_segments * config.segment_length\n",
    "    speedup = standard_ops / hmt_ops\n",
    "    print(f\"  HMT is ~{speedup:.1f}x more efficient while processing MORE tokens!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coverage\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 6))\n",
    "\n",
    "# Standard GPT-2\n",
    "ax1.barh(0, min(article_len, gpt2_limit), height=0.5, \n",
    "         color='blue', alpha=0.7, label='Processed')\n",
    "if article_len > gpt2_limit:\n",
    "    ax1.barh(0, article_len - gpt2_limit, left=gpt2_limit, \n",
    "             height=0.5, color='red', alpha=0.7, label='Truncated (LOST)')\n",
    "ax1.set_xlim(0, article_len)\n",
    "ax1.set_ylim(-0.5, 0.5)\n",
    "ax1.set_xlabel('Token Position')\n",
    "ax1.set_title('Standard GPT-2: Truncates at 1024 tokens')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.set_yticks([])\n",
    "ax1.grid(alpha=0.3, axis='x')\n",
    "\n",
    "# HMT\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(segments_info)))\n",
    "for i, seg in enumerate(segments_info):\n",
    "    ax2.barh(0, seg['length'], left=seg['start'], height=0.5,\n",
    "             color=colors[i], alpha=0.8, edgecolor='black', linewidth=1,\n",
    "             label=f\"Segment {seg['id']}\" if i < 5 else '')\n",
    "    \n",
    "    # Show sensory memory overlap\n",
    "    if seg['sensory'] > 0:\n",
    "        ax2.barh(0, seg['sensory'], left=seg['start'] - seg['sensory'], \n",
    "                height=0.3, color='orange', alpha=0.5)\n",
    "\n",
    "ax2.set_xlim(0, article_len)\n",
    "ax2.set_ylim(-0.5, 0.5)\n",
    "ax2.set_xlabel('Token Position')\n",
    "ax2.set_title('HMT: Processes full article via segmentation + memory')\n",
    "if len(segments_info) <= 5:\n",
    "    ax2.legend(loc='upper right')\n",
    "ax2.set_yticks([])\n",
    "ax2.grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Visualization:\")\n",
    "print(\"  Top: Standard GPT-2 (blue = processed, red = truncated)\")\n",
    "print(\"  Bottom: HMT segments (colored bars) with sensory memory overlap (orange)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding the Three-Level Memory Hierarchy\n",
    "\n",
    "Let's trace what happens during HMT processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_memory_hierarchy(segment_id, config):\n",
    "    \"\"\"\n",
    "    Explain what's in each memory level for a given segment.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ§  Memory Hierarchy at Segment {segment_id}:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Sensory Memory\n",
    "    print(f\"\\n1ï¸âƒ£  SENSORY MEMORY (k={config.sensory_memory_size} tokens):\")\n",
    "    if segment_id == 0:\n",
    "        print(f\"    - EMPTY (first segment, no previous context)\")\n",
    "    else:\n",
    "        print(f\"    - Contains: Last {config.sensory_memory_size} tokens from Segment {segment_id-1}\")\n",
    "        print(f\"    - Purpose: Provides local continuity between segments\")\n",
    "        print(f\"    - Like: Short-term buffer, prevents abrupt context switches\")\n",
    "    \n",
    "    # 2. Short-term Memory\n",
    "    print(f\"\\n2ï¸âƒ£  SHORT-TERM MEMORY (L={config.segment_length} tokens):\")\n",
    "    print(f\"    - Contains: Current segment being processed\")\n",
    "    print(f\"    - Purpose: Active processing by backbone transformer (GPT-2)\")\n",
    "    print(f\"    - Like: Working memory, immediate focus of attention\")\n",
    "    \n",
    "    # 3. Long-term Memory\n",
    "    print(f\"\\n3ï¸âƒ£  LONG-TERM MEMORY (N={config.num_memory_embeddings} embeddings):\")\n",
    "    if segment_id == 0:\n",
    "        print(f\"    - EMPTY (first segment, no history yet)\")\n",
    "    else:\n",
    "        print(f\"    - Contains: {segment_id} memory embeddings from past segments\")\n",
    "        print(f\"    - Purpose: Compressed representation of ALL previous context\")\n",
    "        print(f\"    - Retrieval: Cross-attention finds most relevant {min(segment_id, 5)} memories\")\n",
    "        print(f\"    - Like: Episodic memory, recalls relevant past experiences\")\n",
    "    \n",
    "    # Combined Context\n",
    "    print(f\"\\nðŸ“‹ TOTAL CONTEXT for Segment {segment_id}:\")\n",
    "    sensory_tokens = 0 if segment_id == 0 else config.sensory_memory_size\n",
    "    current_tokens = config.segment_length\n",
    "    memory_embeddings = segment_id\n",
    "    \n",
    "    print(f\"    Sensory:    {sensory_tokens:3d} tokens\")\n",
    "    print(f\"    Current:    {current_tokens:3d} tokens\")\n",
    "    print(f\"    Retrieved:  {memory_embeddings:3d} memory embeddings\")\n",
    "    print(f\"    â†’ Effectively has access to information from ALL {segment_id * config.segment_length + current_tokens} past tokens!\")\n",
    "\n",
    "# Explain for different segments\n",
    "for seg_id in [0, 1, 2, len(segments_info)-1]:\n",
    "    if seg_id < len(segments_info):\n",
    "        explain_memory_hierarchy(seg_id, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Device Compatibility Test\n",
    "\n",
    "Let's verify everything works on your device (MPS/CUDA/CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move batch to device\n",
    "print(f\"ðŸ’» Testing on device: {device}\\n\")\n",
    "\n",
    "batch_on_device = {\n",
    "    k: v.to(device) for k, v in batch.items()\n",
    "}\n",
    "\n",
    "print(f\"âœ… Batch moved to {device}\")\n",
    "print(f\"  input_ids device: {batch_on_device['input_ids'].device}\")\n",
    "print(f\"  Shape: {batch_on_device['input_ids'].shape}\")\n",
    "\n",
    "# Simple computation test\n",
    "input_ids = batch_on_device['input_ids']\n",
    "attention_mask = batch_on_device['attention_mask']\n",
    "\n",
    "# Count tokens per article (respecting padding)\n",
    "tokens_per_article = attention_mask.sum(dim=1)\n",
    "\n",
    "print(f\"\\n  Computation test:\")\n",
    "print(f\"    Tokens per article: {tokens_per_article.tolist()}\")\n",
    "print(f\"    Computed on: {tokens_per_article.device}\")\n",
    "\n",
    "print(f\"\\nðŸš€ Data pipeline is ready for HMT training on {device}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways & Next Steps\n",
    "\n",
    "**What you've learned:**\n",
    "\n",
    "1. **Data Pipeline:**\n",
    "   - `WikiTextDataset` loads and tokenizes articles without truncation\n",
    "   - `LongContextDataLoader` batches variable-length sequences efficiently\n",
    "   - Your pipeline preserves full articles for long-context processing\n",
    "\n",
    "2. **HMT Segmentation:**\n",
    "   - Long articles are split into manageable segments (L=512 tokens)\n",
    "   - Each segment is processed independently by the backbone\n",
    "   - Memory mechanisms connect segments to maintain full context\n",
    "\n",
    "3. **Three-Level Memory:**\n",
    "   - **Sensory:** Last k=32 tokens for local continuity\n",
    "   - **Short-term:** Current L=512 token segment being processed\n",
    "   - **Long-term:** N=300 compressed embeddings from all past segments\n",
    "\n",
    "4. **Efficiency:**\n",
    "   - Standard transformers: O(LÂ²) attention, must truncate\n",
    "   - HMT: O(L) per segment, can process unlimited length\n",
    "\n",
    "**Next Steps:**\n",
    "1. âœ… Data pipeline is ready!\n",
    "2. ðŸš§ Implement the three memory components:\n",
    "   - `RepresentationEncoder` - Summarize segments\n",
    "   - `MemorySearch` - Retrieve relevant memories\n",
    "   - `MemoryEmbeddingGenerator` - Create long-term memories\n",
    "3. ðŸš§ Connect components in `HMT.forward()`\n",
    "4. ðŸš§ Train on WikiText-103!\n",
    "\n",
    "Ready to build the memory components? ðŸ§ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optional: Explore Your Own Examples\n",
    "\n",
    "Try modifying the config and see how it affects segmentation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment: What if we use different segment lengths?\n",
    "\n",
    "for L in [256, 512, 1024]:\n",
    "    config_test = HMTConfig(segment_length=L, sensory_memory_size=32)\n",
    "    num_segs = (len(article_0_clean) + L - 1) // L\n",
    "    \n",
    "    print(f\"\\nSegment length = {L}:\")\n",
    "    print(f\"  Number of segments: {num_segs}\")\n",
    "    print(f\"  Complexity: {num_segs} Ã— O({L}) = O({num_segs * L})\")\n",
    "    print(f\"  Trade-off: {'Fewer segments, more compute per segment' if L >= 512 else 'More segments, less compute per segment'}\")\n",
    "\n",
    "print(\"\\nðŸ¤” Question: What's the optimal segment length?\")\n",
    "print(\"   Answer: Depends on backbone model capacity and memory constraints!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
